{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3a2ba32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:32:58.499012Z",
     "start_time": "2023-11-14T16:32:58.495888Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time as time1\n",
    "import core\n",
    "#from spinup.utils.logx import EpochLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9403bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:03.265957Z",
     "start_time": "2023-11-14T16:33:03.259807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.23.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c042e65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:13.960157Z",
     "start_time": "2023-11-14T16:33:13.957872Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c07090",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:14.390287Z",
     "start_time": "2023-11-14T16:33:14.275983Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_examples\n",
    "import envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88698551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:14.515856Z",
     "start_time": "2023-11-14T16:33:14.504888Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size) #batch_size 만큼 ReplayBuffer(D)에서 random sampling\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2338db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:14.718961Z",
     "start_time": "2023-11-14T16:33:14.709781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-ed01e9aa3c47>:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  initial_state = np.column_stack((A(time).astype('float32'),B(time).astype('float32')))\n"
     ]
    }
   ],
   "source": [
    "Fs = 10\n",
    "N = 500\n",
    "trans_info = [[{'coeff': 1.0, 'amps': 5 * np.array([-0.0154, -0.011, -0.005414]), 'freqs': [6.48e-3, 5.622e-3, 3.7e-3]},\n",
    "               {'coeff': 0.01, 'amps': 5 * np.array([0.01237, -0.005816, -0.03928]), 'freqs': [0.01758, 0.005925, 0.002652]}],\n",
    "              [{'coeff': -0.01, 'amps': 5 * np.array([-0.01568, -0.01486, -0.0064]), 'freqs': [0.00737, 0.0072, -0.0039]},\n",
    "               {'coeff': 1.0, 'amps': 5 * np.array([-0.0154, -0.011, -0.005414]), 'freqs': [6.48e-3, 5.622e-3, 3.7e-3]}]]\n",
    "A = envelope.flattop(t0=5.,len=10.,amp=1.,w=1.)\n",
    "B = envelope.flattop(t0=15.,len=30.,amp=0.8,w=1.)\n",
    "time = np.arange(N) / Fs  # (ns) \n",
    "freq = np.fft.fftfreq(N, d=1/Fs) # (GHz)\n",
    "initial_state = np.column_stack((A(time).astype('float32'),B(time).astype('float32'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91edf76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:14.906709Z",
     "start_time": "2023-11-14T16:33:14.893469Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(id='gym_examples/Quantum', Fs=Fs, N=N, initial_state=initial_state,trans_info=trans_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ca6ad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:15.085224Z",
     "start_time": "2023-11-14T16:33:15.079121Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d45da63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T19:31:08.926550Z",
     "start_time": "2023-11-14T19:31:08.923476Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape #(500,2)\n",
    "act_dim = env.action_space.shape      #(500,2)\n",
    "replay_size = int(1e6)\n",
    "#Action limit\n",
    "act_limit = env.action_space.high[0] #?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9f120e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:17.298701Z",
     "start_time": "2023-11-14T16:33:17.234590Z"
    }
   },
   "outputs": [],
   "source": [
    "ac = core.MLPActorCritic(env.observation_space, env.action_space)\n",
    "ac_targ = deepcopy(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "896b17f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T19:16:03.096040Z",
     "start_time": "2023-11-14T19:16:03.092221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of MLPActorCritic(\n",
       "  (pi): MLPActor(\n",
       "    (pi): Sequential(\n",
       "      (0): Linear(in_features=500, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=500, bias=True)\n",
       "      (5): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (q): MLPQFunction(\n",
       "    (q): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (5): Identity()\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb4799",
   "metadata": {},
   "source": [
    "# 저렇게 만든 ac 에 대한 설명\n",
    "\n",
    "밑에서 for p in ac_targ.parameters() 이지랄한게 \n",
    "ac_targ 결국 nn.Module을 subclassing한 MLPActorCritic(nn.Module)이니까 이 안에 있는 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66694f34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:18.466554Z",
     "start_time": "2023-11-14T16:33:18.463699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze target networks with respect to optimizers (only update ia polyak averaging)\n",
    "for p in ac_targ.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "# 그냥 모든 weight and bias 가져와서 update 꺼버리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f14cc589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:19.501707Z",
     "start_time": "2023-11-14T16:33:19.498887Z"
    }
   },
   "outputs": [],
   "source": [
    "#Experience buffer\n",
    "replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fc7b1b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:20.275333Z",
     "start_time": "2023-11-14T16:33:20.270518Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG Q-loss\n",
    "# 여기서 done 대신 terminated 씀\n",
    "def compute_loss_q(data):\n",
    "    o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "    \n",
    "    q = ac.q(o,a)\n",
    "    \n",
    "    #Bellman backup for Q function (target) \n",
    "    with torch.no_grad():\n",
    "        q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "        backup = r + gamma * (1-d) * q_pi_targ\n",
    "        \n",
    "    loss_q = ((q - backup)**2).mean()\n",
    "    \n",
    "    return loss_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f29df0cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:47:14.480275Z",
     "start_time": "2023-11-14T16:47:14.477190Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up function for computing DDPG pi loss\n",
    "def compute_loss_pi(data):\n",
    "    o = data['obs']\n",
    "    q_pi = ac.q(o, ac.pi(o))\n",
    "    return -q_pi.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60ad3ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T17:23:36.080817Z",
     "start_time": "2023-11-14T17:23:36.077645Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up optimizers for policy and q-function\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "q_optimizer = Adam(ac.q.parameters(), lr=q_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37edc2c",
   "metadata": {},
   "source": [
    "# Set up model saving\n",
    "\n",
    "logger 없이 어떻게 하지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18a56a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T17:25:12.925721Z",
     "start_time": "2023-11-14T17:25:12.920608Z"
    }
   },
   "outputs": [],
   "source": [
    "def update(data):\n",
    "# First run one gradient descent step for Q.\n",
    "    q_optimizer.zero_grad()\n",
    "    loss_q, loss_info = compute_loss_q(data)\n",
    "    loss_q.backward()\n",
    "    q_optimizer.step()\n",
    "\n",
    "    # Freeze Q-network so you don't waste computational effort \n",
    "    # computing gradients for it during the policy learning step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Next run one gradient descent step for pi.\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "    for p in ac.q.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    # Record things\n",
    "    #logger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n",
    "\n",
    "    # Finally, update target networks by polyak averaging.\n",
    "    with torch.no_grad():\n",
    "        for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "            # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "            # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "            p_targ.data.mul_(polyak)\n",
    "            p_targ.data.add_((1 - polyak) * p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c30e9eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:33:22.218891Z",
     "start_time": "2023-11-14T16:33:22.214538Z"
    }
   },
   "outputs": [],
   "source": [
    "#ddpg 내부에서 정의된 parameter\n",
    "seed = 0\n",
    "steps_per_epoch = 4000\n",
    "epochs = 100\n",
    "replay_size = int(1e6)\n",
    "gamma = 0.99\n",
    "polyak = 0.995\n",
    "pi_lr = 1e-3\n",
    "q_lr = 1e-3\n",
    "batch_size = 100\n",
    "start_steps = 10000\n",
    "update_after = 1000\n",
    "update_every = 50\n",
    "act_noise = 0.1\n",
    "max_ep_len = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f54eba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:46:12.810892Z",
     "start_time": "2023-11-14T16:46:12.807819Z"
    }
   },
   "outputs": [],
   "source": [
    "#Prepare for interaction with environment\n",
    "total_steps = steps_per_epoch * epochs\n",
    "#start_time = time1.time()\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74774ac6",
   "metadata": {},
   "source": [
    "# 지금부터는 main  loop 입니당 호호호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff49a8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T16:46:16.814861Z",
     "start_time": "2023-11-14T16:46:16.771134Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cdb35a93155d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mep_len\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mmax_ep_len\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d5d1ec7a6ffd>\u001b[0m in \u001b[0;36mstore\u001b[0;34m(self, obs, act, rew, next_obs, done)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_buf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs2_buf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_buf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "for t in range(total_steps):\n",
    "    \n",
    "    if t > start_steps:\n",
    "        a = get_action(o, act_noise)\n",
    "    else:\n",
    "        a = env.action_space.sample()\n",
    "    \n",
    "    # Step the env   d : terminated, _ :truncated=False, i : info={} \n",
    "    o2, r, d, _, i = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    \n",
    "    # Ignoring done if 강제로 종료당하면 d = False 유지\n",
    "    d = False if ep_len==max_ep_len else d\n",
    "    \n",
    "    replay_buffer.store(o, a, r, o2, d)\n",
    "    \n",
    "    o = o2\n",
    "    \n",
    "    # End of trajectory handling\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        #logger.store(EpRet = ep_ret, Eplen = ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    # Update handling\n",
    "    if t >= update_after and t % udpate_every == 0:\n",
    "        for _ in range(update_every):\n",
    "            batch = replay_buffer.sample_batch(batch_size)\n",
    "            update(data=batch)\n",
    "    \n",
    "    # End of epoch handling\n",
    "    if (t+1) % steps_per_epoch == 0:\n",
    "        epoch = (t+1) // steps_per_epoch\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dec85b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
