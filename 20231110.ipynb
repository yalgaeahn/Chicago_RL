{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10917f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.370557Z",
     "start_time": "2023-11-10T23:51:50.398824Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import envelope\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398cd9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.374940Z",
     "start_time": "2023-11-10T23:51:51.372439Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef74954",
   "metadata": {},
   "source": [
    "# Initial state generator 이거 오늘 만들어야겠다\n",
    "\n",
    "envelope.py \n",
    "flattop(t0, len, amp, w) 이렇게 4개 넣어줘야함\n",
    "\n",
    "A = envelope.flattop(t0=5.,len=10.,amp=1.,w=1.)\n",
    "B = envelope.flattop(t0=15.,len=30.,amp=0.8,w=1.)\n",
    "\n",
    "state_generator()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2029021c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.404143Z",
     "start_time": "2023-11-10T23:51:51.376394Z"
    }
   },
   "outputs": [],
   "source": [
    "Fs = 10\n",
    "N = 500\n",
    "trans_info = [[{'coeff': 1.0, 'amps': 5 * np.array([-0.0154, -0.011, -0.005414]), 'freqs': [6.48e-3, 5.622e-3, 3.7e-3]},\n",
    "               {'coeff': 0.01, 'amps': 5 * np.array([0.01237, -0.005816, -0.03928]), 'freqs': [0.01758, 0.005925, 0.002652]}],\n",
    "              [{'coeff': -0.01, 'amps': 5 * np.array([-0.01568, -0.01486, -0.0064]), 'freqs': [0.00737, 0.0072, -0.0039]},\n",
    "               {'coeff': 1.0, 'amps': 5 * np.array([-0.0154, -0.011, -0.005414]), 'freqs': [6.48e-3, 5.622e-3, 3.7e-3]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a38ac641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.419134Z",
     "start_time": "2023-11-10T23:51:51.406189Z"
    }
   },
   "outputs": [],
   "source": [
    "A = envelope.flattop(t0=5.,len=10.,amp=1.,w=1.)\n",
    "B = envelope.flattop(t0=15.,len=30.,amp=0.8,w=1.)\n",
    "time = np.arange(N) / Fs  # (ns) \n",
    "freq = np.fft.fftfreq(N, d=1/Fs) # (GHz)\n",
    "initial_state = np.column_stack((A(time),B(time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ba38bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.445901Z",
     "start_time": "2023-11-10T23:51:51.420766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/gymnasium/envs/registration.py:481: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(id='gym_examples/Quantum', Fs=Fs, N=N, initial_state=initial_state,trans_info=trans_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78771c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.452126Z",
     "start_time": "2023-11-10T23:51:51.447448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (500, 2), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888dd6aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.575254Z",
     "start_time": "2023-11-10T23:51:51.454760Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Box' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f4c406caa424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Box' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env.action_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103d695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.577480Z",
     "start_time": "2023-11-10T23:51:49.830Z"
    }
   },
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8c7e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.578096Z",
     "start_time": "2023-11-10T23:51:49.832Z"
    }
   },
   "outputs": [],
   "source": [
    "env.action_space[0].high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abd019",
   "metadata": {},
   "source": [
    "### 작동하는데 문제 없어 보이는데 바로 ddpg test\n",
    "\n",
    "### ㅅㅂ ㅈ같은 epoch logger 없이할거임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafb681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.578903Z",
     "start_time": "2023-11-10T23:51:49.835Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a87c1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T20:23:45.234510Z",
     "start_time": "2023-11-10T20:23:45.231153Z"
    }
   },
   "source": [
    " $\\mathcal{B}=\\lbrace(s,a,r,s',d)\\rbrace$ from $\\mathcal{D}$   \n",
    " off policy learning하고 있음을 까먹지 말자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225cb57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.579575Z",
     "start_time": "2023-11-10T23:51:49.837Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size) #batch_size 만큼 ReplayBuffer(D)에서 random sampling\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa3aa8e",
   "metadata": {},
   "source": [
    "# 내가 직접 ddpg를 짜야 할 것 같음\n",
    "그럴려면 얘가 어떻게 돌아가는지 이해해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61390833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.580274Z",
     "start_time": "2023-11-10T23:51:49.840Z"
    }
   },
   "outputs": [],
   "source": [
    "def ddpg(env_fn, actor_critic=core.MLPActorCritic, ac_kwargs=dict(), seed=0, \n",
    "         steps_per_epoch=4000, epochs=100, replay_size=int(1e6), gamma=0.99, \n",
    "         polyak=0.995, pi_lr=1e-3, q_lr=1e-3, batch_size=100, start_steps=10000, \n",
    "         update_after=1000, update_every=50, act_noise=0.1, num_test_episodes=10, \n",
    "         max_ep_len=1000, logger_kwargs=dict(), save_freq=1):\n",
    "    \"\"\"\n",
    "    Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "\n",
    "        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n",
    "            method, a ``pi`` module, and a ``q`` module. The ``act`` method and\n",
    "            ``pi`` module should accept batches of observations as inputs,\n",
    "            and ``q`` should accept a batch of observations and a batch of \n",
    "            actions as inputs. When called, these should return:\n",
    "\n",
    "            ===========  ================  ======================================\n",
    "            Call         Output Shape      Description\n",
    "            ===========  ================  ======================================\n",
    "            ``act``      (batch, act_dim)  | Numpy array of actions for each \n",
    "                                           | observation.\n",
    "            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n",
    "                                           | given observations.\n",
    "            ``q``        (batch,)          | Tensor containing the current estimate\n",
    "                                           | of Q* for the provided observations\n",
    "                                           | and actions. (Critical: make sure to\n",
    "                                           | flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n",
    "            you provided to DDPG.\n",
    "\n",
    "        seed (int): Seed for random number generators.\n",
    "\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "\n",
    "        epochs (int): Number of epochs to run and train agent.\n",
    "\n",
    "        replay_size (int): Maximum length of replay buffer.\n",
    "\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "\n",
    "        polyak (float): Interpolation factor in polyak averaging for target \n",
    "            networks. Target networks are updated towards main networks \n",
    "            according to:\n",
    "\n",
    "            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n",
    "                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n",
    "\n",
    "            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n",
    "            close to 1.)\n",
    "\n",
    "        pi_lr (float): Learning rate for policy.\n",
    "\n",
    "        q_lr (float): Learning rate for Q-networks.\n",
    "\n",
    "        batch_size (int): Minibatch size for SGD.\n",
    "\n",
    "        start_steps (int): Number of steps for uniform-random action selection,\n",
    "            before running real policy. Helps exploration.\n",
    "\n",
    "        update_after (int): Number of env interactions to collect before\n",
    "            starting to do gradient descent updates. Ensures replay buffer\n",
    "            is full enough for useful updates.\n",
    "\n",
    "        update_every (int): Number of env interactions that should elapse\n",
    "            between gradient descent updates. Note: Regardless of how long \n",
    "            you wait between updates, the ratio of env steps to gradient steps \n",
    "            is locked to 1.\n",
    "\n",
    "        act_noise (float): Stddev for Gaussian exploration noise added to \n",
    "            policy at training time. (At test time, no noise is added.)\n",
    "\n",
    "        num_test_episodes (int): Number of episodes to test the deterministic\n",
    "            policy at the end of each epoch.\n",
    "\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    torch.manual_seed(seed) \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env, test_env = env_fn(), env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # Action limit for clamping: critically, assumes all dimensions share the same bound!\n",
    "    act_limit = env.action_space.high[0]\n",
    "\n",
    "    # Create actor-critic module and target networks\n",
    "    ac = actor_critic(env.observation_space, env.action_space, **ac_kwargs)\n",
    "    ac_targ = deepcopy(ac)\n",
    "\n",
    "    # Freeze target networks with respect to optimizers (only update via polyak averaging)\n",
    "    for p in ac_targ.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Experience buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim=obs_dim, act_dim=act_dim, size=replay_size)\n",
    "\n",
    "    # Count variables (protip: try to get a feel for how different size networks behave!)\n",
    "    var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.q])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t q: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up function for computing DDPG Q-loss\n",
    "    def compute_loss_q(data):\n",
    "        o, a, r, o2, d = data['obs'], data['act'], data['rew'], data['obs2'], data['done']\n",
    "\n",
    "        q = ac.q(o,a)\n",
    "\n",
    "        # Bellman backup for Q function\n",
    "        with torch.no_grad():\n",
    "            q_pi_targ = ac_targ.q(o2, ac_targ.pi(o2))\n",
    "            backup = r + gamma * (1 - d) * q_pi_targ\n",
    "\n",
    "        # MSE loss against Bellman backup\n",
    "        loss_q = ((q - backup)**2).mean()\n",
    "\n",
    "        # Useful info for logging\n",
    "        loss_info = dict(QVals=q.detach().numpy())\n",
    "\n",
    "        return loss_q, loss_info\n",
    "\n",
    "    # Set up function for computing DDPG pi loss\n",
    "    def compute_loss_pi(data):\n",
    "        o = data['obs']\n",
    "        q_pi = ac.q(o, ac.pi(o))\n",
    "        return -q_pi.mean()\n",
    "\n",
    "    # Set up optimizers for policy and q-function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    q_optimizer = Adam(ac.q.parameters(), lr=q_lr)\n",
    "\n",
    "    # Set up model saving\n",
    "    logger.setup_pytorch_saver(ac)\n",
    "\n",
    "    def update(data):\n",
    "        # First run one gradient descent step for Q.\n",
    "        q_optimizer.zero_grad()\n",
    "        loss_q, loss_info = compute_loss_q(data)\n",
    "        loss_q.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        # Freeze Q-network so you don't waste computational effort \n",
    "        # computing gradients for it during the policy learning step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Next run one gradient descent step for pi.\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Unfreeze Q-network so you can optimize it at next DDPG step.\n",
    "        for p in ac.q.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        # Record things\n",
    "        logger.store(LossQ=loss_q.item(), LossPi=loss_pi.item(), **loss_info)\n",
    "\n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        with torch.no_grad():\n",
    "            for p, p_targ in zip(ac.parameters(), ac_targ.parameters()):\n",
    "                # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "                # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "                p_targ.data.mul_(polyak)\n",
    "                p_targ.data.add_((1 - polyak) * p.data)\n",
    "\n",
    "    def get_action(o, noise_scale):\n",
    "        a = ac.act(torch.as_tensor(o, dtype=torch.float32))\n",
    "        a += noise_scale * np.random.randn(act_dim)\n",
    "        return np.clip(a, -act_limit, act_limit)\n",
    "\n",
    "    def test_agent():\n",
    "        for j in range(num_test_episodes):\n",
    "            o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0\n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                o, r, d, _ = test_env.step(get_action(o, 0))\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "            logger.store(TestEpRet=ep_ret, TestEpLen=ep_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa2c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.580872Z",
     "start_time": "2023-11-10T23:51:49.842Z"
    }
   },
   "outputs": [],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e08397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.581722Z",
     "start_time": "2023-11-10T23:51:49.844Z"
    }
   },
   "outputs": [],
   "source": [
    "import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187a1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.582367Z",
     "start_time": "2023-11-10T23:51:49.847Z"
    }
   },
   "outputs": [],
   "source": [
    "actor_critic = core.MLPActor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ebc49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.583105Z",
     "start_time": "2023-11-10T23:51:49.849Z"
    }
   },
   "outputs": [],
   "source": [
    "actor_critic(env.observation_space,env.action_space,hidden_sizes=[256,256],activation=nn.ReLU , act_limit=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15654ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.583760Z",
     "start_time": "2023-11-10T23:51:49.852Z"
    }
   },
   "outputs": [],
   "source": [
    "[256]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc47bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.584447Z",
     "start_time": "2023-11-10T23:51:49.854Z"
    }
   },
   "outputs": [],
   "source": [
    "ac_kwargs=dict(hidden_sizes=[args.hid]*args.l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efb8bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.585235Z",
     "start_time": "2023-11-10T23:51:49.857Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs=dict(hidden_sizes=[256]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed7477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.585863Z",
     "start_time": "2023-11-10T23:51:49.859Z"
    }
   },
   "outputs": [],
   "source": [
    "**kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f44f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.586567Z",
     "start_time": "2023-11-10T23:51:49.862Z"
    }
   },
   "outputs": [],
   "source": [
    "env.action_space.high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e31df",
   "metadata": {},
   "source": [
    " ##### self.action_space = spaces.Tuple((spaces.Box(low=-1, high=1, shape=(N, 2)),))\n",
    " ##### self.observation_space = spaces.Tuple((spaces.Box(low=-np.inf, high=np.inf, shape=(N, 2)),))\n",
    " \n",
    " 이거 제대로한게 맞나... 아니여서 spaces.Tuples 제거함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87efc7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:51:51.587175Z",
     "start_time": "2023-11-10T23:51:49.865Z"
    }
   },
   "outputs": [],
   "source": [
    "a =spaces.Box(low=-1, high=1,shape=(N,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15cb7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T23:22:56.287130Z",
     "start_time": "2023-11-10T23:22:56.283159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67762397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
